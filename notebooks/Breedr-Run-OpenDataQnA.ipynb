{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: left;\">\n",
    "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
    "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Open Data QnA**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook assumes you have already Setup Vector Store and Variables are assigned in Config.ini file\n",
    "\n",
    "\n",
    "The notebook covers the following steps: \n",
    "\n",
    "> 1. Take user question and generate sql in the dialect corresponding to data source\n",
    "\n",
    "> 2. Execute the sql query and retreive the data\n",
    "\n",
    "> 3. Generate natural language respose and charts to display\n",
    "\n",
    "> 4. Clean Up resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import logging\n",
    "format_string = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter(format_string)\n",
    "fhandler.setFormatter(formatter)\n",
    "#logger.addHandler(fhandler)\n",
    "logging.basicConfig(format=format_string,\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "logger.setLevel(logging.DEBUG)## ðŸš§ **0. Pre-requisites**\n",
    "\n",
    "Make sure that you have completed the intial setup process using [1_SetUpVectorStore.ipynb](1_SetUpVectorStore.ipynb). If the 1_SetUpVectorStore notebook has been run successfully, the following are set up:\n",
    "* GCP project and all the required IAM permissions\n",
    "\n",
    "* Environment to run the solution\n",
    "\n",
    "* Data source and Vector store for the solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "format_string = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter(format_string)\n",
    "fhandler.setFormatter(formatter)\n",
    "#logger.addHandler(fhandler)\n",
    "logging.basicConfig(format=format_string,\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ **1. Retrieve Configuration Parameters**\n",
    "The notebook will load all the configuration parameters from the `config.ini` file in the root directory. \n",
    "Most of these parameters were set in the initial notebook `1_SetUpVectorStore.ipynb` and save to the 'config.ini file.\n",
    "Use the below cells to retrieve these values and specify additional ones required for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(module_path+'/config.ini')\n",
    "\n",
    "PROJECT_ID = config['GCP']['PROJECT_ID']\n",
    "DATA_SOURCE = config['CONFIG']['DATA_SOURCE']\n",
    "VECTOR_STORE = config['CONFIG']['VECTOR_STORE']\n",
    "\n",
    "BQ_OPENDATAQNA_DATASET_NAME = config['BIGQUERY']['BQ_OPENDATAQNA_DATASET_NAME']\n",
    "BQ_LOG_TABLE_NAME = config['BIGQUERY']['BQ_LOG_TABLE_NAME'] \n",
    "BQ_DATASET_REGION = config['BIGQUERY']['BQ_DATASET_REGION']\n",
    "BQ_DATASET_NAME = config['BIGQUERY']['BQ_DATASET_NAME']\n",
    "BQ_TABLE_LIST = config['BIGQUERY']['BQ_TABLE_LIST']\n",
    "\n",
    "#The Postgress settings are not used, but some of the API calls below depend on them being set.\n",
    "PG_SCHEMA = config['PGCLOUDSQL']['PG_SCHEMA']\n",
    "PG_DATABASE = config['PGCLOUDSQL']['PG_DATABASE']\n",
    "PG_USER = config['PGCLOUDSQL']['PG_USER']\n",
    "PG_REGION = config['PGCLOUDSQL']['PG_REGION'] \n",
    "PG_INSTANCE = config['PGCLOUDSQL']['PG_INSTANCE'] \n",
    "PG_PASSWORD = config['PGCLOUDSQL']['PG_PASSWORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” **2. Authenticate and Connect to Google Cloud Project**\n",
    "Authenticate to Google Cloud as the IAM user logged into this notebook in order to access your Google Cloud Project.\n",
    "\n",
    "You can do this within Google Colab or using the Application Default Credentials in the Google Cloud CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Colab Auth\"\"\" \n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "\n",
    "\"\"\"Google CLI Auth\"\"\"\n",
    "# !gcloud auth application-default login\n",
    "\n",
    "\n",
    "import google.auth\n",
    "credentials, project_id = google.auth.default()\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "print(f'Project has been set to {PROJECT_ID}')\n",
    "!gcloud auth application-default set-quota-project {PROJECT_ID}\n",
    "\n",
    "import os\n",
    "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ï¸ **3. Run the Open Data QnA Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”— **3A. Connect to Datasource, Vector Source and Vertex AI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the USER_DATABASE based on data source\n",
    "from dbconnectors import pgconnector, bqconnector\n",
    "if DATA_SOURCE=='bigquery':\n",
    "    USER_DATABASE=BQ_DATASET_NAME \n",
    "    src_connector = bqconnector\n",
    "else: \n",
    "    USER_DATABASE=PG_SCHEMA\n",
    "    src_connector = pgconnector\n",
    "\n",
    "print(\"Source selected is : \"+ str(DATA_SOURCE) + \"\\nSchema or Dataset Name is : \"+ str(USER_DATABASE))\n",
    "print(\"Vector Store selected is : \"+ str(VECTOR_STORE))\n",
    "\n",
    "\n",
    "\n",
    "# Set the vector store paramaters\n",
    "if VECTOR_STORE=='bigquery-vector':\n",
    "    region=BQ_DATASET_REGION\n",
    "    vector_connector = bqconnector\n",
    "    call_await = False\n",
    "\n",
    "else:\n",
    "    region=PG_REGION\n",
    "    vector_connector = pgconnector\n",
    "    call_await=True\n",
    "\n",
    "print(f'Region is {region}')\n",
    "      \n",
    "num_table_matches = 5\n",
    "num_column_matches = 10\n",
    "similarity_threshold = 0.3\n",
    "num_sql_matches=3\n",
    "\n",
    "\n",
    "RUN_DEBUGGER = True \n",
    "EXECUTE_FINAL_SQL = True \n",
    "\n",
    "from google.api_core.exceptions import NotFound\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "from agents import EmbedderAgent, BuildSQLAgent, DebugSQLAgent, ValidateSQLAgent, ResponseAgent, VisualizeAgent\n",
    "\n",
    "\n",
    "embedder = EmbedderAgent('vertex') \n",
    "\n",
    "llm_model = \"gemini-1.5-flash-001\"\n",
    "SQLBuilder = BuildSQLAgent(llm_model)\n",
    "SQLChecker = ValidateSQLAgent(llm_model)\n",
    "SQLDebugger = DebugSQLAgent(llm_model)\n",
    "Responder = ResponseAgent(llm_model)\n",
    "Visualize = VisualizeAgent ()\n",
    "\n",
    "found_in_vector = 'N'\n",
    "final_sql='Not Generated Yet'\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=region)\n",
    "aiplatform.init(project=PROJECT_ID, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  â“ **3B. Ask your Natural Language Question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def answer_question(user_question):\n",
    "    # Fetch the embedding of the user's input question \n",
    "    embedded_question = embedder.create(user_question)\n",
    "\n",
    "    # Reset AUDIT_TEXT\n",
    "    AUDIT_TEXT = ''\n",
    "\n",
    "    AUDIT_TEXT = AUDIT_TEXT + \"\\nUser Question : \" + str(user_question) + \"\\nUser Database : \" + str(USER_DATABASE)\n",
    "    process_step = \"\\n\\nGet Exact Match: \"\n",
    "    # Look for exact matches in known questions \n",
    "    exact_sql_history = vector_connector.getExactMatches(user_question) \n",
    "\n",
    "    if exact_sql_history is not None:\n",
    "        found_in_vector = 'Y' \n",
    "        final_sql = exact_sql_history\n",
    "        invalid_response = False\n",
    "        AUDIT_TEXT = AUDIT_TEXT + \"\\nExact match has been found! Going to retreive the SQL query from cache and serve!\"\n",
    "\n",
    "\n",
    "    else:\n",
    "        found_in_vector = 'N'\n",
    "        # No exact match found. Proceed looking for similar entries in db \n",
    "        AUDIT_TEXT = AUDIT_TEXT +  process_step + \"\\nNo exact match found in query cache, retreiving revelant schema and known good queries for few shot examples using similarity search....\"\n",
    "        process_step = \"\\n\\nGet Similar Match: \"\n",
    "        if call_await:\n",
    "            similar_sql = await vector_connector.getSimilarMatches('example', USER_DATABASE, embedded_question, num_sql_matches, similarity_threshold)\n",
    "        else:\n",
    "            similar_sql = vector_connector.getSimilarMatches('example', USER_DATABASE, embedded_question, num_sql_matches, similarity_threshold)\n",
    "\n",
    "        process_step = \"\\n\\nGet Table and Column Schema: \"\n",
    "        # Retrieve matching tables and columns\n",
    "        if call_await: \n",
    "            table_matches =  await vector_connector.getSimilarMatches('table', USER_DATABASE, embedded_question, num_table_matches, similarity_threshold)\n",
    "            column_matches =  await vector_connector.getSimilarMatches('column', USER_DATABASE, embedded_question, num_column_matches, similarity_threshold)\n",
    "        else:\n",
    "            table_matches =  vector_connector.getSimilarMatches('table', USER_DATABASE, embedded_question, num_table_matches, similarity_threshold)\n",
    "            column_matches =  vector_connector.getSimilarMatches('column', USER_DATABASE, embedded_question, num_column_matches, similarity_threshold)\n",
    "\n",
    "        AUDIT_TEXT = AUDIT_TEXT +  process_step + \"\\nRetrieved Similar Known Good Queries, Table Schema and Column Schema: \\n\" + '\\nRetrieved Tables: \\n' + str(table_matches) + '\\n\\nRetrieved Columns: \\n' + str(column_matches) + '\\n\\nRetrieved Known Good Queries: \\n' + str(similar_sql)\n",
    "        # If similar table and column schemas found: \n",
    "        if len(table_matches.replace('Schema(values):','').replace(' ','')) > 0 or len(column_matches.replace('Column name(type):','').replace(' ','')) > 0 :\n",
    "\n",
    "            # GENERATE SQL\n",
    "            process_step = \"\\n\\nBuild SQL: \"\n",
    "            generated_sql = SQLBuilder.build_sql(DATA_SOURCE,user_question,table_matches,column_matches,similar_sql)\n",
    "            final_sql=generated_sql\n",
    "            AUDIT_TEXT = AUDIT_TEXT + process_step +  \"\\nGenerated SQL : \" + str(generated_sql)\n",
    "            \n",
    "            if 'unrelated_answer' in generated_sql :\n",
    "                invalid_response=True\n",
    "\n",
    "            # If agent assessment is valid, proceed with checks  \n",
    "            else:\n",
    "                invalid_response=False\n",
    "\n",
    "                if RUN_DEBUGGER: \n",
    "                    generated_sql, invalid_response, AUDIT_TEXT = SQLDebugger.start_debugger(DATA_SOURCE, generated_sql, user_question, SQLChecker, table_matches, column_matches, AUDIT_TEXT, similar_sql) \n",
    "                    # AUDIT_TEXT = AUDIT_TEXT + '\\n Feedback from Debugger: \\n' + feedback_text\n",
    "\n",
    "                final_sql=generated_sql\n",
    "                AUDIT_TEXT = AUDIT_TEXT + \"\\nFinal SQL after Debugger: \\n\" +str(final_sql)\n",
    "\n",
    "\n",
    "        # No matching table found \n",
    "        else:\n",
    "            invalid_response=True\n",
    "            print('No tables found in Vector ...')\n",
    "            AUDIT_TEXT = AUDIT_TEXT + \"\\nNo tables have been found in the Vector DB. The question cannot be answered with the provide data source!\"\n",
    "\n",
    "    print(f'\\n\\n AUDIT_TEXT: \\n {AUDIT_TEXT}')\n",
    "\n",
    "\n",
    "    if not invalid_response:\n",
    "        try: \n",
    "            if EXECUTE_FINAL_SQL is True:\n",
    "                    final_exec_result_df=src_connector.retrieve_df(final_sql.replace(\"```sql\",\"\").replace(\"```\",\"\").replace(\"EXPLAIN ANALYZE \",\"\"))\n",
    "                    print('\\nQuestion: ' + user_question + '\\n')\n",
    "                    # print('\\n Final SQL Execution Result: \\n')\n",
    "                    # print(final_exec_result_df)\n",
    "                    response = final_exec_result_df\n",
    "                    _resp=Responder.run(user_question, response)\n",
    "                    AUDIT_TEXT = AUDIT_TEXT + \"\\nModel says \" + str(_resp) \n",
    "\n",
    "\n",
    "            else:  # Do not execute final SQL\n",
    "                    print(\"Not executing final SQL since EXECUTE_FINAL_SQL variable is False\\n \")\n",
    "                    response = \"Please enable the Execution of the final SQL so I can provide an answer\"\n",
    "                    _resp=Responder.run(user_question, response)\n",
    "                    AUDIT_TEXT = AUDIT_TEXT + \"\\nModel says \" + str(_resp) \n",
    "\n",
    "        except ValueError: \n",
    "            print('')\n",
    "        # except Exception as e: \n",
    "        #     print(f\"An error occured. Aborting... Error Message: {e}\")\n",
    "            \n",
    "    else:  # Do not execute final SQL\n",
    "        print(\"Not executing final SQL as it is invalid, please debug!\")\n",
    "        response = \"I am sorry, I could not come up with a valid SQL.\"\n",
    "        _resp=Responder.run(user_question, response)\n",
    "        AUDIT_TEXT = AUDIT_TEXT + \"\\nModel says \" + str(_resp)\n",
    "\n",
    "    print(\"Final Answer:\" + str(_resp))\n",
    "    bqconnector.make_audit_entry(DATA_SOURCE, USER_DATABASE, llm_model, user_question, final_sql, found_in_vector, \"\", process_step, \"\", AUDIT_TEXT)  \n",
    "\n",
    "    return  str(_resp)\n",
    "\n",
    "\n",
    "def chart_answer(response):\n",
    "    chart_js=''\n",
    "    chart_js = Visualize.generate_charts(user_question,final_sql,response) #sending \n",
    "    # print(chart_js[\"chart_div_1\"])\n",
    "\n",
    "    html_code = f'''\n",
    "    <script type=\"text/javascript\" src=\"https://www.gstatic.com/charts/loader.js\"></script>\n",
    "    <script type=\"text/javascript\">\n",
    "    {chart_js[\"chart_div_1\"]}\n",
    "    </script>\n",
    "    <div id=\"chart_div_1\"></div>\n",
    "    '''\n",
    "\n",
    "    HTML(html_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\033[1mData Source:- \"+ DATA_SOURCE)\n",
    "print(\"Vector Store:- \"+ VECTOR_STORE)\n",
    "print(\"Schema:- \"+USER_DATABASE)\n",
    "    \n",
    "# Suggested question for 'fda_food' dataset: \"What are the top 5 cities with highest recalls?\"\n",
    "#  Suggested question for 'google_dei' dataset: \"How many asian men were part of the leadership workforce in 2021?\"\n",
    "\n",
    "prompt_for_question = \"Please enter your question for source :\" + DATA_SOURCE + \" and database : \" + USER_DATABASE\n",
    "#user_question = input(prompt_for_question) #Uncomment if you want to ask question yourself\n",
    "user_question = 'what is the date of birth for animal UK286760601653?' # Or Enter Question here\n",
    "\n",
    "print(f\"Ask database {USER_DATABASE} the {user_question}\")\n",
    "\n",
    "response = await answer_question(user_question)\n",
    "print(f\"{response=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question= \"Which are my best cows? analyse the data to find which cattle have the  highest rates of growth, and have been on the farm for the longest time.  \"\n",
    "print(f\"Ask database {USER_DATABASE} the {user_question}\")\n",
    "\n",
    "response = await answer_question(user_question)\n",
    "print(f\"{response=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Charts for the results (Run only when you have proper results in the above cells)\n",
    "Agent provides two suggestive google charts to display on a UI with element IDs chart_div and chart_div_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chart_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{response=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendataqna import Visualize\n",
    "\n",
    "chart_js=''\n",
    "chart_js = Visualize.generate_charts(user_question,final_sql,response) #sending \n",
    "# print(chart_js[\"chart_div_1\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "html_code = f'''\n",
    "<script type=\"text/javascript\" src=\"https://www.gstatic.com/charts/loader.js\"></script>\n",
    "<script type=\"text/javascript\">\n",
    "{chart_js[\"chart_div\"]}\n",
    "</script>\n",
    "<div id=\"chart_div\"></div>\n",
    "'''\n",
    "\n",
    "HTML(html_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "opendataqna",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "opendataqna",
   "language": "python",
   "name": "opendataqna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
